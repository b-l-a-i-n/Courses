{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnbirlR4rGd1"
      },
      "outputs": [],
      "source": [
        "!pip install gym\n",
        "!pip install gym-maze-trustycoder83"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_maze\n",
        "import numpy as np \n",
        "import random\n",
        "import time\n",
        "import os\n",
        "# Create video device\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2c-GcdhrTeQ",
        "outputId": "17d0b414-fff1-4db7-f449-fc1dce59ea07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x8 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create environment"
      ],
      "metadata": {
        "id": "BETp8Obmr5sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('maze-sample-5x5-v0')\n",
        "state_n = 25\n",
        "action_n = 4"
      ],
      "metadata": {
        "id": "5RdjFPXhsAgt"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating state"
      ],
      "metadata": {
        "id": "fgHHdgdpzTWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_state(obs):\n",
        "    return int(obs[1] * np.sqrt(state_n) + obs[0])"
      ],
      "metadata": {
        "id": "UpCztQ6uzRwW"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "ToYVUfsH1ZUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent():\n",
        "    def __init__(self, action_n):\n",
        "        self.action_n = action_n\n",
        "        return None\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        return random.randint(0, self.action_n - 1)"
      ],
      "metadata": {
        "id": "q--Qe69_1axC"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross enthropy method"
      ],
      "metadata": {
        "id": "RwFxgTyN4-Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CEM():\n",
        "    def __init__(self, state_n, action_n):\n",
        "        self.state_n = state_n\n",
        "        self.action_n = action_n\n",
        "        self.policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        return int(np.random.choice(np.arange(self.action_n), p=self.policy[state]))\n",
        "    \n",
        "    def update_policy(self, elite_trajectories):\n",
        "        pre_policy = np.zeros((self.state_n, self.action_n))\n",
        "        \n",
        "        for trajectory in elite_trajectories:\n",
        "            for state, action in zip(trajectory['states'], trajectory['actions']):\n",
        "                pre_policy[state][action] += 1\n",
        "                \n",
        "        for state in range(self.state_n):\n",
        "            if sum(pre_policy[state]) == 0:\n",
        "                self.policy[state] = np.ones(self.action_n) / self.action_n\n",
        "            else:\n",
        "                self.policy[state] = pre_policy[state] / sum(pre_policy[state])\n",
        "                \n",
        "        return None"
      ],
      "metadata": {
        "id": "TuiowFZw5DNh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating agent tragectory"
      ],
      "metadata": {
        "id": "-z4Nv5AOy5jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trajectory(agent, trajectory_len):\n",
        "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
        "    \n",
        "    obs = env.reset()\n",
        "    state = get_state(obs)\n",
        "    trajectory['states'].append(state)\n",
        "    \n",
        "    for _ in range(trajectory_len):\n",
        "        \n",
        "        action = agent.get_action(state)\n",
        "        trajectory['actions'].append(action)\n",
        "        \n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        state = get_state(obs)\n",
        "        trajectory['total_reward'] += reward\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        trajectory['states'].append(state)\n",
        "            \n",
        "    return trajectory\n"
      ],
      "metadata": {
        "id": "1fYpkcpayO8b"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Elite traectiries"
      ],
      "metadata": {
        "id": "7tA40D5_-y4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elite_trajectories(trajectories, q_param):\n",
        "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
        "    quantile = np.quantile(total_rewards, q=q_param) \n",
        "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]"
      ],
      "metadata": {
        "id": "fgB573Xg-yYw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "AVBZh8KO191o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random agent"
      ],
      "metadata": {
        "id": "M1JfPdSgAKiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RandomAgent(action_n)\n",
        "trajectory = get_trajectory(agent, trajectory_len=20)\n",
        "print(trajectory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlZ3p0sO1_Fa",
        "outputId": "b0d169a1-507e-41d4-f05b-805167330c14"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'states': [0, 1, 2, 2, 3, 3, 2, 2, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], 'actions': [3, 3, 0, 3, 3, 2, 1, 2, 0, 2, 2, 1, 2, 2, 3, 2, 2, 3, 2, 3], 'total_reward': -0.08000000000000004}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-entropy agent"
      ],
      "metadata": {
        "id": "sJnGw1DtANjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = CEM(state_n, action_n)\n",
        "episode_n = 50\n",
        "trajectory_n = 100\n",
        "trajectory_len = 100\n",
        "q_param = 0.9\n",
        "\n",
        "\n",
        "for _ in range(episode_n):\n",
        "    trajectories = [get_trajectory(agent, trajectory_len) for _ in range(trajectory_n)]\n",
        "    \n",
        "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
        "    print(mean_total_reward)\n",
        "    \n",
        "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
        "    \n",
        "    if len(elite_trajectories) > 0:\n",
        "        agent.update_policy(elite_trajectories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8WZukfh8w29",
        "outputId": "e9aa4caa-c5b3-4428-8dc1-01169479a049"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.37932000000000016\n",
            "0.3755999999999997\n",
            "0.7961199999999998\n",
            "0.8821599999999996\n",
            "0.92124\n",
            "0.9362\n",
            "0.9350800000000001\n",
            "0.9363999999999999\n",
            "0.9367599999999999\n",
            "0.9359599999999999\n",
            "0.9356799999999998\n",
            "0.93548\n",
            "0.9359599999999998\n",
            "0.9359199999999999\n",
            "0.9355999999999998\n",
            "0.9358\n",
            "0.9357999999999999\n",
            "0.9351599999999999\n",
            "0.9365200000000001\n",
            "0.9347599999999998\n",
            "0.9361999999999998\n",
            "0.9356799999999998\n",
            "0.9368799999999997\n",
            "0.9359599999999998\n",
            "0.9357599999999999\n",
            "0.9351599999999999\n",
            "0.93536\n",
            "0.9354799999999998\n",
            "0.93604\n",
            "0.93648\n",
            "0.9358799999999999\n",
            "0.9363999999999999\n",
            "0.9364400000000002\n",
            "0.9361200000000003\n",
            "0.93612\n",
            "0.93612\n",
            "0.93628\n",
            "0.9363199999999998\n",
            "0.9358399999999999\n",
            "0.9358\n",
            "0.9352399999999997\n",
            "0.9361599999999999\n",
            "0.9359199999999999\n",
            "0.9354399999999998\n",
            "0.9355199999999999\n",
            "0.9357599999999998\n",
            "0.9357999999999999\n",
            "0.93556\n",
            "0.9361999999999998\n",
            "0.9365599999999997\n"
          ]
        }
      ]
    }
  ]
}